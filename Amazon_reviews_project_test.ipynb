{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>23</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HiveContext as 'sqlContext'\n",
      "SparkContext and HiveContext created. Executing user code ...\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SQLContext, SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "import matplotlib\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- helpfuless_count: long (nullable = true)\n",
      " |-- helpfuless_score: long (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- profileName: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- userId: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.json(\"In/reviews.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+-----+\n",
      "|helpfuless_score|helpfuless_count|   price|score|\n",
      "+----------------+----------------+--------+-----+\n",
      "|               7|               7| unknown|  4.0|\n",
      "|               0|               0|   17.99|  5.0|\n",
      "|               0|               1|   17.99|  3.0|\n",
      "|               7|               7| unknown|  4.0|\n",
      "|               3|               4|   15.99|  5.0|\n",
      "+----------------+----------------+--------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df1 = df.select('helpfuless_score', 'helpfuless_count','price','score')\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------+-----+\n",
      "|helpfuless_score|helpfuless_count| price|score|\n",
      "+----------------+----------------+------+-----+\n",
      "|               3|               4| 15.99|  5.0|\n",
      "|               8|              10| 19.40|  5.0|\n",
      "|               1|               1| 19.40|  5.0|\n",
      "|               1|               1| 19.40|  5.0|\n",
      "|               1|               1| 19.40|  5.0|\n",
      "|               4|               4| 10.26|  5.0|\n",
      "|               1|               1| 10.26|  5.0|\n",
      "|               7|              11| 10.95|  1.0|\n",
      "|               1|               2| 10.95|  4.0|\n",
      "|               1|               2| 10.95|  1.0|\n",
      "|               2|               4| 10.95|  5.0|\n",
      "|               5|               9| 10.95|  5.0|\n",
      "|               1|               3| 10.95|  5.0|\n",
      "|               1|               4| 10.95|  5.0|\n",
      "|               1|               4| 10.95|  5.0|\n",
      "|               4|               6| 10.95|  5.0|\n",
      "|               2|               3| 10.95|  5.0|\n",
      "|               1|               5| 10.95|  4.0|\n",
      "|               8|               8| 10.95|  5.0|\n",
      "|               5|               5| 10.95|  1.0|\n",
      "+----------------+----------------+------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "df1 = df1[df1.helpfuless_score > 0]\n",
    "df1 = df1[df1.helpfuless_count > 0]\n",
    "df1 = df1[df1.price > 0]\n",
    "df1 = df1[df1.score > 0]\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+----------------+\n",
      "|score| price|helpfuless_count|helpfuless_score|\n",
      "+-----+------+----------------+----------------+\n",
      "|  5.0| 15.99|               4|               3|\n",
      "|  5.0| 19.40|              10|               8|\n",
      "|  5.0| 19.40|               1|               1|\n",
      "|  5.0| 19.40|               1|               1|\n",
      "|  5.0| 19.40|               1|               1|\n",
      "+-----+------+----------------+----------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "df1 = df1.select('score', 'price','helpfuless_count','helpfuless_score')\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feature column: {u' 3.0': 3, u' 4.0': 4, u' 1.0': 2, u' 2.0': 0, u' 5.0': 1}"
     ]
    }
   ],
   "source": [
    "print \"Mapping of first categorical feature column: %s\" % get_mapping(df1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 5\n",
      "Feature vector length for numerical features: 1\n",
      "Total feature vector length: 6"
     ]
    }
   ],
   "source": [
    "mappings = [get_mapping(df1, i) for i in range(0,1)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(df1.first()[1:2])\n",
    "total_len = num_len + cat_len\n",
    "\n",
    "print \"Feature vector length for categorical features: %d\" % cat_len\n",
    "print \"Feature vector length for numerical features: %d\" % num_len\n",
    "print \"Total feature vector length: %d\" % total_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[0:0]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    num_vec = np.array([float(field) for field in record[1:2]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "def extract_label(record):\n",
    "    return float(record[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df1.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: (u' 5.0', u' 15.99', 4, 3)\n",
      "Label: 3.0\n",
      "Linear Model feature vector:\n",
      "[0.0,0.0,0.0,0.0,0.0,15.99]\n",
      "Linear Model feature vector length: 6"
     ]
    }
   ],
   "source": [
    "first = df1.first()\n",
    "first_point = data.first()\n",
    "print \"Raw data: \" + str(first[0:])\n",
    "print \"Label: \" + str(first_point.label)\n",
    "print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
    "print \"Linear Model feature vector length: \" + str(len(first_point.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>28</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HiveContext as 'sqlContext'\n",
      "SparkContext and HiveContext created. Executing user code ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "name 'df1' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df1' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_features_dt(record):\n",
    "    return np.array(map(float, record[0:2]))\n",
    "data_dt = df1.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
    "first_point_dt = data_dt.first()\n",
    "print \"Decision Tree feature vector: \" + str(first_point_dt.features)\n",
    "print \"Decision Tree feature vector length: \" + str(len(first_point_dt.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAM edit below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_model = DecisionTree.trainRegressor(data_dt,{})\n",
    "preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
    "actual = data.map(lambda p: p.label)\n",
    "true_vs_predicted_dt = actual.zip(preds)\n",
    "print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))\n",
    "print \"Decision Tree depth: \" + str(dt_model.depth())\n",
    "print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)\n",
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_error(actual, pred):\n",
    "    return (pred - actual)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
    "print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
    "print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt\n",
    "print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt\n",
    "print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
