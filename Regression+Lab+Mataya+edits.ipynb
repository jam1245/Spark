{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4 - Lab 1: Building a Regression Model with Spark\n",
    "\n",
    "This lab will build upon the previous lab. \n",
    "While classification models deal with outcomes that represent discrete classes, regression models are concerned with target variables that can take any real value. Our goal is to find a model that maps input features to predicted target variables. \n",
    "Like classification, regression is also a form of supervised learning.\n",
    "\n",
    "We can use regression models to predict a mulititude of different variables of interest. \n",
    "Here are some examples of how this technique is used in society:\n",
    "* Predicting stock returns and other economic variables\n",
    "* Predicting loss amounts for loan defaults (this can be combined with a classification model that predicts the probability of default, while the regression model predicts the amount in the case of a default)\n",
    "* Predicting customer lifetime value (CLTV) in a retail, mobile, or other business, based on user behavior and spending patterns\n",
    "\n",
    "Below, we will fire up our Spark cluster and prepare it for data visualization like we have done in previous labs. See Module 3 - Lab 1: Feature-Extraction-MoreContext for a detailed description of what is happening below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>270</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HiveContext as 'sqlContext'\n",
      "SparkContext and HiveContext created. Executing user code ...\n",
      "Ready"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "\n",
    "# Make Random String\n",
    "import os, random, string\n",
    "length = 32\n",
    "chars = string.ascii_letters + string.digits + '_-'\n",
    "random.seed = (os.urandom(1024))\n",
    "\n",
    "rndfolder = ''.join(random.choice(chars) for i in range(length))\n",
    "dirpath = '/home/hadoop/work_dir/' + rndfolder + '/'\n",
    "\n",
    "# Set Path and permissions (\"0770\", which means everyone can read and write the file)\n",
    "os.mkdir(dirpath, 0770)\n",
    "os.chdir(dirpath)\n",
    "\n",
    "def process_figure(fig, name):\n",
    "    fig.savefig(name)\n",
    "    print 'http://ec2-54-153-99-19.us-west-1.compute.amazonaws.com:8810/' + rndfolder + '/' + name\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('agg') # non-graphical mode (pngs(?))\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of regression models\n",
    "Let's talk about the different types of regression models before we dive into coding up an example.\n",
    "Spark's MLlib library offers two broad classes of regression models: linear models and decision tree regression models.\n",
    "Linear regression models use a different loss function, related link function, and decision function than its classification counterparts.\n",
    "(We will go over these functions in minute.)\n",
    "MLlib provides a standard least squares regression model, which is a type of a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least square regression\n",
    "There is a variety of loss functions that can be applied to generalized linear models. \n",
    "(A loss function is a function that maps an event (or values of one or more variables) onto a real number intuitively representing some \"cost\" associated with the event.)\n",
    "\n",
    "This is the loss function used for least squares (a.k.a. the squared loss):\n",
    "    1/2[(w^T) * x - y]^2\n",
    "* y: the target variable (it has a real value)\n",
    "* w: weight vector\n",
    "* x: feature vector\n",
    "* T: instance? \n",
    "\n",
    "So what is this useful for? \n",
    "What does all of this actually mean?\n",
    "Least Squares Regression is the method for summarizing a pattern in a dataset that suggests a linear relationship.\n",
    "This allows us to predict an outcome, y, for a given input, x. \n",
    "(i.e. We can describe (or predict) how a response variable, y, changes as an explanatory variable, x, changes.)\n",
    "This model is useful for SPECIFIC situations.\n",
    "The standard least squares regression in MLlib does not use regularization (which is a process of introducing additional information in order to solve an ill-posed problem or to prevent overfitting).\n",
    "We can see that the loss applied to incorrectly predicted points will be magnified since the loss is squared.\n",
    "This means that least squares regression is susceptible to outliers in the dataset and also to over-fitting.\n",
    "So, generally, we should apply some level of regularization to account for this.\n",
    "\n",
    "Here are some things that may happen using this technique against the data (so if you aren't sure what the data looks like and you use this method, the following situations could occur):\n",
    "* A curved pattern might appear showing that the relationship is not linear\n",
    "* Increasing or decreasing spread about the line as x increases indicates that prediction of y will be LESS accurate for larger x's.\n",
    "* Individual points with large residuals are outliers  in the vertical direction\n",
    "* Individual points that are extreme in the x direction are also important....as influential observations\n",
    "This link is helpful for explaining this concept even further: http://www.henry.k12.ga.us/ugh/apstat/chapternotes/sec3.3.html\n",
    "(I recommend reading it. It's not too long, and I think it is helpful to explain the math behind the method.)\n",
    "\n",
    "The link function is the identity link.\n",
    "The decision function is also the identity function, since generally, no thresholding is applied in regression. \n",
    "Therefore, the model's prediction is simply: y = (w^T) * x\n",
    "\n",
    "Linear regression with L2 regularization is commonly referred to as ridge regression, while applying L1 regularization is called the lasso.\n",
    "To understand this concept better, read the \"Regularization\" section of this website: http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees for regression\n",
    "The Decision tree method builds regression or classification models in the form of a tree structure. \n",
    "It brakes down a dataset into smaller and smaller subsets while incrementally developing an associated decision tree at the same time.\n",
    "The final result is a tree with decision nodes and leaf nodes.\n",
    "\n",
    "* A decision node has two or more branches each representing values for the attribute being tested.\n",
    "* A leaf node represents a decision on the numerical target. \n",
    "* The topmost node is called the \"root node\".\n",
    "\n",
    "Decision trees can handle both categorical and numerical data.\n",
    "A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).\n",
    "We use standard deviation to calculate the homogeneity of a numerical sample.\n",
    "If the numerical sample is completely homogeneous its standard deviation is zero.\n",
    "The standard deviation reduction is based on the decrease in standard deviation after a dataset is split on an attribute. \n",
    "Constructing a decision tree is all about finding attribute that returns the highest standard deviation reduction (i.e., the most homogeneous branches).\n",
    "I recommend reading this content for a more detailed explanation of this concept: \n",
    "http://chem-eng.utoronto.ca/~datamining/dmc/decision_tree_reg.htm\n",
    "\n",
    "Just like using linear models for regression tasks involves changing the loss function used, using decision trees for regression involves changing the measure of the node impurity used.\n",
    "The impurity metric is called variance and is defined in the same way as the squared loss for least squares linear regression.\n",
    "\n",
    "## Extracting the right features from you data\n",
    "As the underlying models for regression are the same as those for the classification case, we can use the same approach to create input features. \n",
    "The only practical difference is that the target is now a real-valued variable, as opposed to a categorical one. \n",
    "The `LabeledPoint` class in MLlib already takes this into account, as the label field is of the `Double` type, so it can handle both cases.\n",
    "\n",
    "### Extracting features from the bike sharing dataset\n",
    "To illustrate the concepts in this chapter, we will be using the bike sharing dataset. \n",
    "This dataset contains hourly records of the number of bicycle rentals in the capital bike sharing system. \n",
    "It also contains variables related to date and time, weather, and seasonal and holiday information.\n",
    "\n",
    "Here are the variable names and descriptions:\n",
    "\n",
    "* `instant`: This is the record ID\n",
    "* `dteday`: This is the raw data\n",
    "* `season`: This is different seasons such as spring, summer, winter, and fall\n",
    "* `yr`: This is the year (2011 or 2012)\n",
    "* `mnth`: This is the month of the year\n",
    "* `hr`: This is the hour of the day\n",
    "* `holiday`: This is whether the day was a holiday or not\n",
    "* `weekday`: This is the day of the week\n",
    "* `workingday`: This is whether the day was a working day or not\n",
    "* `weathersit`: This is a categorical variable that describes the weather at a particular time\n",
    "* `temp`: This is the normalized temperature\n",
    "* `atemp`: This is the normalized apparent temperature\n",
    "* `hum`: This is the normalized humidity\n",
    "* `windspeed`: This is the normalized wind speed\n",
    "* `cnt`: This is the target variable, that is, the count of bike rentals for that hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']\n",
      "17379"
     ]
    }
   ],
   "source": [
    "path = \"bike/hour_noheader.csv\"\n",
    "raw_data = sc.textFile(path)\n",
    "num_data = raw_data.count()\n",
    "records = raw_data.map(lambda x: x.split(\",\"))\n",
    "first = records.first()\n",
    "print first\n",
    "print num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code above loads the \"bike/hour_noheader.csv\" dataset into the Spark instance that is created (similar to previous datasets that we have used in earlier modules).\n",
    "The data is then counted in the line `num_data = raw_data.count()`. \n",
    "The data is then parsed in this line `records = raw_data.map(lambda x: x.split(\",\"))`. \n",
    "\n",
    "* The lambda expression splits each line in the data set by the comma (,) character and returns the list to be stored in the `records` variable.\n",
    "\n",
    "The first line in the records dataset is selected in this line `first = records.first()`.\n",
    "It is then printed and along with the number of records that was previously counted. \n",
    "As we can see from the results, we have 17,379 hourly records in our dataset. \n",
    "For now, we will ignore the record ID and raw date columns.\n",
    "We will also ignore the `casual` and `registered` count target variables and focus on the overall count variable, `cnt` (which is the sum of the other two counts).\n",
    "We are left with 12 variables. \n",
    "The first eight are categorical, while the last 4 are normalized real-valued variables.\n",
    "\n",
    "To deal with the eight categorical variables, we will use the binary encoding approach.\n",
    "The four real-valued variables will be left as is.\n",
    "\n",
    "First, we will cache the dataset since we will be reading from it a lot in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[4] at RDD at PythonRDD.scala:43"
     ]
    }
   ],
   "source": [
    "records.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "In order to extract each categorical feature into a binary vector form, we will need to know the feature mapping of each feature value to the index of the nonzero value in our binary vector.\n",
    "So this means....what?\n",
    "\n",
    "* We need to use a binary vector to store the codes for the eight categorical variabes the we get from our binary encoding method. (We haven't done this yet.)\n",
    "* To perform this method, we have to know how each feature maps to an index within the binary vector. \n",
    "\n",
    "Let's define a function that will extract this mapping from our dataset for a given column.\n",
    "The function below will do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_mapping(rdd, idx)` first maps the field to its unique values. \n",
    "Then, it uses the `zipWithIndex()` transformation to \"zip\" the value up with a unique index such that a key-value RDD is formed.\n",
    "In the formulated RDD, the key is the variable and the value is the index.\n",
    "\n",
    "* This index will be the index of the nonzero entry in the binary vector representation of the feature.\n",
    "\n",
    "We then collect this RDD back to the driver as a Python dictionary via the `collectAsMap()` function.\n",
    "\n",
    "Let's test our function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feature column: {u'1': 0, u'3': 1, u'2': 2, u'4': 3}"
     ]
    }
   ],
   "source": [
    "print \"Mapping of first categorical feature column: %s\" % get_mapping(records, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply the function to each categorical column (i.e. for variable indices 2 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mappings = [get_mapping(records, i) for i in range(2,10)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(records.first()[11:15])\n",
    "total_len = num_len + cat_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the mappings for each variable (as seen in the code above), and we can see how many values in total we need for our binary vector representation in the code below.\n",
    "\n",
    "In the code block above:\n",
    "\n",
    "* `mappings = [get_mapping(records, i) for i in range(2,10)]` obtains the mappings that we desire for each variable.\n",
    "* `cat_len = sum(map(len, mappings))` obtains the length of each variable's mapping, and then sums them.\n",
    "* `num_len = len(records.first()[11:15])` obtains the length of the values in the 11th-15th indices in the first list in records\n",
    "    * `first()` gets the first item from the `records` list\n",
    "* `total_len = num_len + cat_len` calculates the total length of the values, which will be necessary for our tree construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 57\n",
      "Feature vector length for numerical features: 4\n",
      "Total feature vector length: 61"
     ]
    }
   ],
   "source": [
    "print \"Feature vector length for categorical features: %d\" % cat_len\n",
    "print \"Feature vector length for numerical features: %d\" % num_len\n",
    "print \"Total feature vector length: %d\" % total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
