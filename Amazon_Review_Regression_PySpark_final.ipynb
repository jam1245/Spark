{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkContext as 'sc'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>16</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating HiveContext as 'sqlContext'\n",
      "SparkContext and HiveContext created. Executing user code ...\n"
     ]
    }
   ],
   "source": [
    "#Initialize\n",
    "from pyspark import SQLContext, SparkContext, SparkConf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "import matplotlib\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- helpfuless_count: long (nullable = true)\n",
      " |-- helpfuless_score: long (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- profileName: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- userId: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "#Print Schema\n",
    "df = sqlContext.read.json(\"In/reviews.json\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------+-----+\n",
      "|helpfuless_score|helpfuless_count|   price|score|\n",
      "+----------------+----------------+--------+-----+\n",
      "|               7|               7| unknown|  4.0|\n",
      "|               0|               0|   17.99|  5.0|\n",
      "|               0|               1|   17.99|  3.0|\n",
      "|               7|               7| unknown|  4.0|\n",
      "|               3|               4|   15.99|  5.0|\n",
      "+----------------+----------------+--------+-----+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "#Show Sample of Initial Values\n",
    "df1 = df.select('helpfuless_score', 'helpfuless_count','price','score')\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+------+-----+\n",
      "|helpfuless_score|helpfuless_count| price|score|\n",
      "+----------------+----------------+------+-----+\n",
      "|               3|               4| 15.99|  5.0|\n",
      "|               8|              10| 19.40|  5.0|\n",
      "|               1|               1| 19.40|  5.0|\n",
      "|               1|               1| 19.40|  5.0|\n",
      "|               1|               1| 19.40|  5.0|\n",
      "|               4|               4| 10.26|  5.0|\n",
      "|               1|               1| 10.26|  5.0|\n",
      "|               7|              11| 10.95|  1.0|\n",
      "|               1|               2| 10.95|  4.0|\n",
      "|               1|               2| 10.95|  1.0|\n",
      "|               2|               4| 10.95|  5.0|\n",
      "|               5|               9| 10.95|  5.0|\n",
      "|               1|               3| 10.95|  5.0|\n",
      "|               1|               4| 10.95|  5.0|\n",
      "|               1|               4| 10.95|  5.0|\n",
      "|               4|               6| 10.95|  5.0|\n",
      "|               2|               3| 10.95|  5.0|\n",
      "|               1|               5| 10.95|  4.0|\n",
      "|               8|               8| 10.95|  5.0|\n",
      "|               5|               5| 10.95|  1.0|\n",
      "+----------------+----------------+------+-----+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "#Remove 0s and unknowns\n",
    "df1 = df1[df1.helpfuless_score > 0]\n",
    "df1 = df1[df1.helpfuless_count > 0]\n",
    "df1 = df1[df1.price > 0]\n",
    "df1 = df1[df1.score > 0]\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----------------+----------------+\n",
      "|score| price|helpfuless_count|helpfuless_score|\n",
      "+-----+------+----------------+----------------+\n",
      "|  5.0| 15.99|               4|               3|\n",
      "|  5.0| 19.40|              10|               8|\n",
      "|  5.0| 19.40|               1|               1|\n",
      "|  5.0| 19.40|               1|               1|\n",
      "|  5.0| 19.40|               1|               1|\n",
      "+-----+------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "DataFrame[score: string, price: string, helpfuless_count: bigint, helpfuless_score: bigint]"
     ]
    }
   ],
   "source": [
    "#Check out the 'cleansed' dataset and cache (if helpful)\n",
    "df1 = df1.select('score', 'price','helpfuless_count','helpfuless_score')\n",
    "df1.show(5)\n",
    "df1.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define mapping function\n",
    "def get_mapping(rdd, idx):\n",
    "    return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping of first categorical feature column: {u' 3.0': 3, u' 4.0': 4, u' 1.0': 2, u' 2.0': 0, u' 5.0': 1}"
     ]
    }
   ],
   "source": [
    "#Print mapping of Score variable\n",
    "print \"Mapping of first categorical feature column: %s\" % get_mapping(df1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vector length for categorical features: 5\n",
      "Feature vector length for numerical features: 2\n",
      "Total feature vector length: 7"
     ]
    }
   ],
   "source": [
    "#Perform mapping - find lengths of categorical and numerical vectors\n",
    "#Score will be flattened out into categorical\n",
    "#Price and Helpfulness Count will be numerical\n",
    "mappings = [get_mapping(df1, i) for i in range(0,1)]\n",
    "cat_len = sum(map(len, mappings))\n",
    "num_len = len(df1.first()[1:3])\n",
    "total_len = num_len + cat_len\n",
    "\n",
    "print \"Feature vector length for categorical features: %d\" % cat_len\n",
    "print \"Feature vector length for numerical features: %d\" % num_len\n",
    "print \"Total feature vector length: %d\" % total_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define extract_features and extract_label functions\n",
    "def extract_features(record):\n",
    "    cat_vec = np.zeros(cat_len)\n",
    "    i = 0\n",
    "    step = 0\n",
    "    for field in record[0:1]:\n",
    "        m = mappings[i]\n",
    "        idx = m[field]\n",
    "        cat_vec[idx + step] = 1\n",
    "        i = i + 1\n",
    "        step = step + len(m)\n",
    "    num_vec = np.array([float(field) for field in record[1:3]])\n",
    "    return np.concatenate((cat_vec, num_vec))\n",
    "def extract_label(record):\n",
    "    return float(record[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create LabeledPoint RDD object based on the above functions\n",
    "data = df1.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data: (u' 5.0', u' 15.99', 4, 3)\n",
      "Label: 3.0\n",
      "Linear Model feature vector:\n",
      "[0.0,1.0,0.0,0.0,0.0,15.99,4.0]\n",
      "Linear Model feature vector length: 7"
     ]
    }
   ],
   "source": [
    "#Check out the data\n",
    "first = df1.first()\n",
    "first_point = data.first()\n",
    "print \"Raw data: \" + str(first[0:])\n",
    "print \"Label: \" + str(first_point.label)\n",
    "print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
    "print \"Linear Model feature vector length: \" + str(len(first_point.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([0.0, 1.0, 0.0, 0.0, 0.0, 15.99, 4.0]), DenseVector([0.0, 1.0, 0.0, 0.0, 0.0, 19.4, 10.0]), DenseVector([0.0, 1.0, 0.0, 0.0, 0.0, 19.4, 1.0]), DenseVector([0.0, 1.0, 0.0, 0.0, 0.0, 19.4, 1.0]), DenseVector([0.0, 1.0, 0.0, 0.0, 0.0, 19.4, 1.0])]"
     ]
    }
   ],
   "source": [
    "#Grab features from the RDD object for scaling\n",
    "features = data.map(lambda x: x.features)\n",
    "features.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.3033, 0.1636]), DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.409]), DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.0409]), DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.0409]), DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.0409])]"
     ]
    }
   ],
   "source": [
    "#Load scaling libraries and perform scaling on feature vectors\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "model = standardizer.fit(features)\n",
    "features_transform = model.transform(features)\n",
    "features_transform.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 8, 1, 1, 1]"
     ]
    }
   ],
   "source": [
    "#Get label vector (Helpfulness Score values)\n",
    "lab = df1.map(lambda row: row[3])\n",
    "lab.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.3033, 0.1636])), (8, DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.409])), (1, DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.0409])), (1, DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.0409])), (1, DenseVector([0.0, 2.0134, 0.0, 0.0, 0.0, 0.368, 0.0409]))]"
     ]
    }
   ],
   "source": [
    "#Zip (combine) the scaled features data with the Helpfulness Score data\n",
    "transformedData = lab.zip(features_transform)\n",
    "transformedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(3.0, [0.0,2.01341972154,0.0,0.0,0.0,0.303282603018,0.163614079319]), LabeledPoint(8.0, [0.0,2.01341972154,0.0,0.0,0.0,0.367960131241,0.409035198297]), LabeledPoint(1.0, [0.0,2.01341972154,0.0,0.0,0.0,0.367960131241,0.0409035198297]), LabeledPoint(1.0, [0.0,2.01341972154,0.0,0.0,0.0,0.367960131241,0.0409035198297]), LabeledPoint(1.0, [0.0,2.01341972154,0.0,0.0,0.0,0.367960131241,0.0409035198297])]"
     ]
    }
   ],
   "source": [
    "#Get the RDD back into LabeledPoint format for the linear regression\n",
    "transformedData = transformedData.map(lambda row: LabeledPoint(row[0],[row[1]]))\n",
    "transformedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Run linear regression model\n",
    "linear_model = LinearRegressionWithSGD.train(transformedData, iterations=10, step=0.1, intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model predictions: [(3.0, 47.125492833594983), (8.0, 101.22626963863219), (1.0, 23.6482800840792), (1.0, 23.6482800840792), (1.0, 23.6482800840792)]"
     ]
    }
   ],
   "source": [
    "#Check out Actual vs Predicted values\n",
    "true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
    "print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define error testing functions\n",
    "def squared_error(actual, pred):\n",
    "    return (pred - actual)**2\n",
    "\n",
    "def abs_error(actual, pred):\n",
    "    return np.abs(pred - actual)\n",
    "\n",
    "def squared_log_error(pred, actual):\n",
    "    return (np.log(pred + 1) - np.log(actual + 1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model - Mean Squared Error: 45293.8213\n",
      "Linear Model - Mean Absolute Error: 86.6048\n",
      "Linear Model - Root Mean Squared Log Error: 2.6456"
     ]
    }
   ],
   "source": [
    "#Calculate and display error values of the model\n",
    "mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
    "mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
    "rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
    "print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
    "print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
    "print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python"
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
